\chapter{Conclusion}

\begin{table}[]
\centering
\caption{Results}
\label{resultAll}
\begin{tabular}{|c|c|}
\hline
Tf-IDF   & 0.44 (± 0.04) \\
PVDM(dbow) & 0.40    \\
FastText &  ~0.51   \\
FastText(Pinyin) &  ~0.51  \\
Siamese-CBOW(5) & 0.41 (± 0.04) \\
Siamese-CBOW(10) & 0.39 (± 0.03) \\
Siamese-CBOW(10) with pre-train & 0.45 (± 0.02) \\

\hline
\end{tabular}
\end{table}

\begin{table}[]
\centering
\caption{FastText}
\label{fasttext}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
   & 8 & 12 & 16 & 32 & 64 \\
\hline
no segmentation  & 0.369 & 0.375 & 0.389 & 0.372 & 0.368 \\
segmentation  & 0.515 & 0.515 & 0.514 & 0.516 & 0.513 \\
segmentation + pinyin  & 0.513 & 0.518 & 0.516 & 0.517 & 0.51 \\
\hline
\end{tabular}
\end{table}

\begin{table}[]
\centering
\caption{Result of PVDB}
\label{resultAll}
\begin{tabular}{|c|c|c|c|}
\hline
      & Test set & Training Set \\
\hline
dm/c  & 0.384 &  0.384 \\
dbow &  0.404  & 0.457 \\
dm/m &  0.38  & 0.436 \\
\hline
\end{tabular}
\end{table}


\section{Experiment Settings}


We used baseline TD-IDF plus SVM with linear kernel as baseline. 
Since the original distribution for classes is a little skewed, most the test sample is classified into 2 major classes.
We compared it with other models with different settings. \\

For PVDB, we use 3 different models dm/c and dm/m and dbow. All of them, we choose most commonly-used parameters,dm : dimension:100, window size:10, negative:5, hs:0 and we tested both dm with concatenation of context vectors (dm/c) and average of context vectors(dm/m). 
The other model dbow, we chose the same parameters.

In FastText experiment, we iterated through the parameters like window size from 8 to 100, 
loss function ns,hs,softmax.  Since the result did't indicate significant difference between these parameters, 
we only display 1 of them as reference.

Additionally, we also tried to convert data set to pinyin to evaluate 
if the pinyin improve the sematic recognition for FastText, 
which support vocabulary expansion with subword information \cite{bojanowski2016enriching}. 

\section{Result}

It took around 2 days with GPU NVIDIA TITAN X (PASCAL) to finish Siamese-CBOW word-embedding training. With both 5,10 epoch the perforamance are below the baseline. 
We would discuss in the Discussion. The other models can be complete with CPU within hours.

We also look into the classification result with confusion matrix in Figure \ref{confusion1} and Figure \ref{confusion2}.

That we can see that the 2 major classes get best accuracy while the test results skewed into these 2 major classes as well.  
The ANGER, DISGUST, and FEAR are more likely to be classified as SAD. It is reasonable that those posts contain more negative feeling.
The tendancy in the Siamese-CBOW result is more obvious. And it classify no entry into some rarely-used classes.

In the confusion matrix for fasttext, it shows different tendancy for classification. The accuracy for SURPRICE is higher than SAD.

\begin{figure}[h]
    \centering
	\includegraphics[width=1\linewidth]{tf-idf}
    \caption{The confusion matrix for TF-IDF+ SVM}
    \label{confusion1}
\end{figure}

\begin{figure}[h]
    \centering
	\includegraphics[width=1\linewidth]{sc-ratio}
    \caption{The confusion matrix for two models comparison}
    \label{confusion2}
\end{figure}

\begin{figure}[h]
    \centering
	\includegraphics[width=1\linewidth]{fasttext-cm}
    \caption{The confusion matrix for fasttext}
    \label{confusion3}
\end{figure}