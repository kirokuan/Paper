\chapter{Introduction}
\setlength{\baselineskip}{1.5em}
\setlength{\parindent}{2em}
\setlength{\parskip}{1em}

\section{Abstract}

How to make the sentence embedding with its own semantics more precisely is study of interest
,since it's beneficial for several NLP tasks like machine translation, sentiment analysis. 
Since the internet text volume grows so enormously and rapidly, how to make the information can be extracted more efficiently and precisely become more critical for many applications.  
Chinese forums, blogs or microblog expand especially rapidly. The studies tried to vectorize the sentences with deep learning approach with more general way to make it invariant to the languages properties.  

Recently word2vec\cite{word2vec} is considered to work for evaluating word semantics in general cases.  
Additionally, the character is invariant to the language. Nevertheless, 
the embedding in sentence level is more complicated, it's related to the sentence structure,  
intention or context. There are several methods raised in recent years, like Siamese-CBOW, FastText ...etc. 
Most of them is able to train batch of text and construct the vectors.

\section{Purpose}

So far, most the studies are conducted in English or more general way to applied in various languages
,since The most platforms are contributed by the worldwide users. Most approaches also are aimed at being invariant to language properties. 
However, few of them evaluate the effectiveness of these approaches when it comes to Chinese.   
we are also interested if the feature also works in Chinese or other languages, and if the algorithm is invariant to the language grammar. 