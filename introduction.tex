\chapter{Introduction}
\setlength{\baselineskip}{1.5em}
\setlength{\parindent}{2em}
\setlength{\parskip}{1em}

\section{Abstract}

How to make the sentence embedding with its own semantics more precisely is study of interest,since it's beneficial for several NLP tasks like machine translation, sentiment analysis. Since the internet text volume grows so enormously and rapidly, how to make the information can be extracted more efficiently and precisely become more critical for many application. Chinese forums, blogs or microblog expand especially rapidly, and the articles and the posts are produced.

Recently word2vec, Mikolov et al.(2013a), is considered to work for evaluating word semantics in general cases.  Additionally, the character is invariant to the language. Nevertheless, the embedding in sentence level is more complicated, it's related to the sentence structure,  intention or context. There are several methods raised in recent years, like Siamese-CBOW, FastText ...etc. Most of them is able to train batch of text and construct the vectors.

\section{Purpose}

So far, most the studies are conducted in English, we are also interested if the feature also works in Chinese or other languages, and if the algorithm is invariant to the language grammar. 