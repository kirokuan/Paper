\chapter{Introduction}
\setlength{\baselineskip}{1.5em}
\setlength{\parindent}{2em}
\setlength{\parskip}{1em}

\section{Background}

How to make the sentence embedding with its own semantics more precisely is a study of interest, since it is beneficial for several NLP tasks like machine translation, sentiment analysis. 
The volume of internet text grows so enormously and rapidly, and the new derivative or new word keep growing as well.
Chinese forums, blogs or microblog expand especially rapidly. 
It becomes more critical for many applications to make the information can be extracted more efficiently and precisely.  

Recently word2vec\cite{word2vec} is considered to work for evaluating word semantics in general cases.  
Additionally, the character of word2vec is invariant to the languages. Nevertheless, 
the embedding in sentence or phase level is more complicated, it is related to the sentence structure, intention or context. 
Traditional techniques like n-gram, bag-of-words or parsing from syntax can not overcome some difficulties like high dimension or inability to be generalized enough.  
The recent studies tried to vectorize the sentences with deep learning approaches in more general way and make it invariant to the languages properties.  


Most dataset for NLP work are still in Indo-European Languages, including English, Spanish. Wiki suggested that 46\% people speak Indo-European as their first language. 
Although Indo-European languages are spread widely, there are also other languages like Chinese, Japanese, which own their special properties.
However, the properties of languages are considered little in most publications.

\section{Purpose}

So far, most studies about distributed representation of sentence are conducted mainly in English, or in multilingual environments, which are from the forums, review platforms contributed by the worldwide users. Most techniques also are aimed at being invariant to language properties or applicable to multilingual environment. 
However, few of them evaluate the effectiveness of these techniques to other languages, neither they evaluate the multilingual dataset with considering the characteristic of other languages.   
We are interested if those models also work in Chinese or other languages, and if the algorithm is invariant to the language grammar or language property. 
In this paper, we demonstrate the modern methods on practical data, and compare it with traditional methods.