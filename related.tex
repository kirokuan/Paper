\chapter{Related Work}


\section{Traditional Approach}

K. Dashtipour et al \cite{Dashtipour2016} summarized both corpus-base and lexicon-base techniques purposed recently and listed the languages those techniques aiming at, and there are some innovative mothods combined both approaches. 
The advantage of corpus-base is that it is dictionary-free, but it requires relatively larger corpus to build the model, while lexicon-based approach depends mainly on existing resources to detect the sentiment.
When lexicon-based approach comes to the informal articles contributed by netizens, it may suffer some troubles like misspelling, abbreviation ,words or metaphor...etc, neither can it take the sequence of words into consideration.   

The basic corpus-based approach like TF-IDF is considered to be able to generate relatively good precision.
However, both approaches may utilize some keywords in the sentences rather than sentiment of sentence itself. In real world, we often use negation or irony to present our feeling rather than solely keywords.
To solve the problems from rapidly-evolved languages, there are both semi-supervised and unsupervised approaches introduced as well. 

The another problem from the approach is the sparse matrix and high dimension vector due to the complexity of the language nature. 
Sparse matrix results in inefficience of classifiers and difficulty of scaling up. Additionally, the relationship of synonymy can not be modeled.
A classic method for generating dense vectors is to utilize singular value decomposition, SVD.
SVD is part of a family of methods that can approximate an N-dimensional dataset using fewer dimensions, including Principle Components Analysis (PCA), Factor Analysis, and so on.
Nonetheless, there is a significant computational cost for the SVD for a large cooccurrence matrix, and performance is not always better than using the full sparse vectors.

\section{Chinese Related Sentiment Analysis}

In recent years, most models are aimed at English or more general way. When it comes to multiligual environment, the preprocess approach may differ in languages. The traditional ways to counter the variation of words like stemming or lemmatization are appliable to most Latin languages.
Howerver, in Chinese and Japanese, segmentation may also be invloved. In the example of FastText\cite{joulin2016fasttext}, they also demostrated to convert character into pinyin, which make the subword infomation can be obtained. 

Though most approaches are tested and verified by English dataset, there are some work to test in Chinese dataset as well.

Zhao et al. \cite{zhao2012moodlens} performed the basic way to classify the articles from WeiBo with Naive Bayes and smoothing with Laplace smooth.  
In this work, the authors also use emoticon as the ground truth to verify the approach, it also applied some imcrement learning. \\

%There are also some researches about Emoticon in sentiment analysis like 
%\cite{Emojis}, which indicates the high correlation between emoticon and sentiment from the users. 

\section{Advanced Approach}

Besides the traditional approaches, they try to extract the effective features in statistical way. 
With deep learning approaches, it may be possible to learn more in continuous representations.
Additionally, sentiment analysis with typical deep learning models are conducted. Multiple tasks are performed like parsing (Socher et al., 2013a), language
modeling (Bengio et al., 2003; Mnih and Hinton,2009) and NER (Turian et al., 2010). For sentiment analysis, the legendary models were also tested, like CNN \cite{kim2014convolutional}, RNN \cite{arevian2007recurrent}, but most of them are applied in English dataset only. 

Recently, word2vec(Mikolov et al. 2013) \cite{word2vec} is considered to perform efficiently to vectorize the meaning of single words. It is a log-bilinear model to learn continuous
representations of words on very large corpora efficiently. The similar concept can be adapted to phrase or sentence level as well.
Mikolov also purposed it with sentence level \cite{PVDM} called PVDM, and claimed it can be applied to both short text and long articles. These approaches are unsupervised, but it can be conducted to sentiment analysis with proper transformation. 

The other way to model the semantic is to use encoder-decoder model, which come from statistical machine translation. Skip-thoughts\cite{kiros2015skip} employs the GRU encoder-decoder models.
And it also combines "vocabulary expansion" from \cite{MikolovLS13}, they used vocabulary expansion to map word embeddings into the RNN encoder space.
Therefore, it made it possible to use less vocabulary to build model, and reuse pre-trained model.

To detect the sentiment polarity of short text has attracted the interest of study as well, the most used dataset is informal tweets, which is contributed by the netizens from different background. 
For the task of sentiment classification, an effective feature learning method is to compose the representation of a
sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011).

Pang et al. \cite{pang2002thumbs} already used bag-of-word representations, and present the word as one-hot vector. They get the better classification results. However, it is still not enough to represent the complex meaning or linguistic characteristics.
The following works, like \cite{tang2014learning}, the paper purposed to use deep learning to make sentiment analysis directly.

There is also a work\cite{multilingual} to evaluate the multilingual approaches and monolingual one. However, it used the Spanish and English as target, both two are belongs 
to Indo-European languages. It also addressed the culture difference, "dragon" mean harmful in English but it's opposite in Chinese. 
