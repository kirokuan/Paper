\chapter{Experiment}

\section{Set Up}

\begin{table}[]
\centering
\caption{Tag Category}
\label{CategoryTable}
\begin{tabular}{ll}
      &  \\
JOY  & 呵呵 酷 赞 贊 乐乐 鼓掌 耶 \\
DISGUST & 黑线 汗 晕 \\
SAD &   可憐 淚 衰 失望 伤心 泪 生病 囧 鄙视  淚 衰 失望 伤心 泪 生病 囧 鄙视  \\
FEAR &  委屈  可憐 \\
SURPRISE &  吃驚 吃惊 \\
ANGER & 怒 抓狂
\end{tabular}
\end{table}

\begin{figure}[h]
    \centering
	\includegraphics[width=1\linewidth]{weibo}
    \caption{The emoticons in WeiBo}
    \label{fig:weibo}
\end{figure}

The dataset we chose is Open WeiboScope\cite{fu2013reality}, which is collected WeiBo randomly with API by researchers at the Journalism and Media Center of the University of Hong Kong in 2012. 
It contains 226 millions posts distributing evenly over the year. 
it's a Weibo feature to allow the user to use emoticon, 
and the emoticon in raw data expressed as [笑](smile),[淚](tear). It displays as images like the figure \ref{fig:weibo}. 
We used the tags in posts as the indicators of sentiment, and removed some duplicated posts or some posts without any tags, or too many tags. 
We evaluated the accuracy of the classification for different algorithms. We used the TF-IDF and SVM (Joachims, 1998). as baseline.


\section{Preprocess}

For the data preprocessing and cleansing, most posts contains more than 1 tag. To avoid ambiguity, we only preserve those with single tag.
we removed the posts containing too many tags, or without any tag. We also removed the duplicated posts by their post id roughly because it is a property of Chinese microblog \cite{fu2013reality} for Chinese netizens to post repeatedly. 
Besides, we only chose the post that over certain length (over above 10 characters). Finally, we used jieba and dictionary to segment to post. \\

Like \cite{zhao2012moodlens}, we also suffered the problem that the numbers of emoticon classes skewed. The numbers of JOY and SAD are more than 50\% of posts. 
we deleted some posts from JOY and SAD randomly to make the dataset more balance.  

The posts meets the criteria is about 7.4 millions. And we removed the tags in the original post, and there are so many tags 
,we use most-used 6 categories to categorize them as \ref{table:CategoryTable}.

After initial round, we found some special string or tokens like username or url may affect the result. 
Therefore, we also removed those special tokens from the posts as well.

\section{Other}

In the Paragraph vector experiment, we tested both DM and DBOW. Additionally, there are 2 different ways supported by gensim to use average or concatenation.
And we used the parameters suggested. 

\section{FastText}

In the FastText experiment, we tried 3 formats, including non-segmented dataset, segmented dataset and the dataset with pinyin.
We tested the non-segmented dataset, since some training data in demo is Japanese without segmentation. We wonder if the segmentation matters.
Additionally, we also want to test if the subword information work for Chinese. So we convert the dataset to pinyin as well. We can see the differences from the table\ref{table:ftdataset}.

For converting to pinyin, we use jieba + pinyin (https://www.npmjs.com/package/pinyin) npm package to convert the  charaters to pinyin, which also includes the tone.

\begin{table}[]
\centering
\caption{FastText Dataset}
\label{ftdataset}
\begin{tabular}{|c|c|}
\hline
   & \\
\hline
no segmentation  & 弊喇,好似有少少喉嚨痛添! \\
segmentation  & 弊 喇 , 好似 有 少 少 喉嚨痛 添 ! \\
segmentation + pinyin  & bì lǎ , hǎosì yǒu shǎo shǎo hóulóngtòng tiān ! \\
\hline
\end{tabular}
\end{table}

We also tested the both dimesion size from 8-300, and loss function including hs (Hierarchical Softmax), ns (negative sampling), softmax.
