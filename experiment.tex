\chapter{Experiments}

\section{Experimental Settings}

\begin{table}[]
\centering
\caption{Tag Category}
\label{CategoryTable}
\begin{tabular}{|c|c|}
\hline
JOY  & 呵呵 酷 \begin{CJK}{UTF8}{gbsn}赞 乐乐\end{CJK} 贊 鼓掌 耶 \\
DISGUST & \begin{CJK}{UTF8}{gbsn}黑线 汗 晕\end{CJK} \\
SAD &   可憐 淚 衰 失望 \begin{CJK}{UTF8}{gbsn}伤心 泪\end{CJK} 生病 囧 \begin{CJK}{UTF8}{gbsn}鄙视\end{CJK}  \\
FEAR &  委屈  可憐 \\
SURPRISE &  吃驚  \begin{CJK}{UTF8}{gbsn}吃惊\end{CJK} \\
ANGER & 怒 抓狂 \\
\hline
\end{tabular}
\end{table}

\begin{figure}[h]
    \centering
	\includegraphics[width=1\linewidth]{weibo}
    \caption{The emoticons in WeiBo}
    \label{fig:weibo}
\end{figure}

The dataset we chose is Open WeiboScope\cite{fu2013reality}, which is collected WeiBo randomly with API by researchers at the Journalism and Media Center of the University of Hong Kong in 2012. 
It contains 226 millions posts distributing evenly over the year.  
The most Weibo users come from the different provinces of China. There are also some users from Hong Kong or oversea. 
The content of Weibo contains both simplified Chinese and traditional Chinese. Some province dialect is seen in the dataset.

It is a Weibo feature to allow the user to use emoticon, 
and the emoticon in raw data expressed as [笑](smile), [淚](tear). It displays as images like the Figure \ref{fig:weibo}. 
We used the tags in posts as the indicators of sentiment, and removed some duplicated posts or some posts without any tags, or too many tags. 
We evaluated the accuracy of the classification for different algorithms. We used the TF-IDF and SVM (Joachims, 1998). as baseline.

\section{Preprocess}

For the data preprocessing and cleansing, most posts contain more than one tag. To avoid ambiguity, we only preserve those with the single tag.
We removed the posts containing too many tags, or without any tag. We also removed the duplicated posts by their post id roughly because it is a property of Chinese microblog \cite{fu2013reality} for Chinese netizens to post repeatedly. 
Besides, we only chose the posts over the certain length (over 10 characters). Finally, we used jieba and dictionary to segment to post. 

We used most-used 6 emotion which most social network support: JOY, SAD, ANGER, FEAR, SURPRISE, DISGUST.
We classified these tags into these classes manually. 
Like \cite{zhao2012moodlens}, we also suffered the problem that the numbers of emoticon classes skewed. The numbers of JOY and SAD are more than 50\% of posts. 
We only selected some specific tags from JOY and SAD to make the whole dataset more balance.   
The mapping table shows in Table \ref{CategoryTable}. The JOY contains the tags like 呵呵(haha), 贊(excellent)...etc. 
The SAD contains the tags like 失望(disappointed), 淚(tear).


We removed the tags from the original posts, and there are so many tags. 
The posts left for 6 categories display in Table \ref{cat_num}. 
The classes the most posts belonging to are still JOY and SAD. 

After the initial round, we found some special string or tokens like username or url may affect the result. 
Therefore, we also removed those special tokens from the posts as well.

\begin{table}[]
\centering
\caption{Number for categories}
\label{cat_num}
\begin{tabular}{|c|c|}
\hline
ANGER      &331,091 \\                                                           
DISGUST    &261,955 \\                                                         
FEAR       &151,564 \\                                                         
JOY        &717,059 \\                                                           
SAD        &788,492 \\                                                           
SURPRISE   &191,974 \\
\hline
\end{tabular}
\end{table}


\section{PVDM}

In the Paragraph vector experiment, we tested both DM and DBOW. Additionally, there are 2 different DM supported by Gensim to use average or concatenation.
We use DM/C and DM/M to represent concatenation and average separately and used the parameters suggested for three models. 

The dimension of the vector is 100, and negative-sampling is 5 for both DM and DBOW models, and window size are 5, 10 for concatenation, average separately. 


\section{FastText}

In FastText experiment, we tried three formats, including non-segmented dataset, segmented dataset and the dataset with pinyin.
We tested the non-segmented dataset since some training data in the demonstration is Japanese without segmentation. We wonder if the segmentation matters.
Additionally, we also want to test if the subword information \cite{bojanowski2016enriching} works for Chinese. So we convert the dataset to pinyin as well. We can see the differences from the table\ref{ftdataset}.

For converting to pinyin, we use jieba + pinyin (https://www.npmjs.com/package/pinyin) npm package to convert the characters to pinyin, which also includes the tone.
We used the built-in classifier to classify the test set.

We iterated through the parameters like window size from 8 to 100, loss function ns, hs, softmax.  
Since the result did not indicate significant difference between these parameters, we only display 1 of them as reference. 


\begin{table}[]
\centering
\caption{FestText Dataset}
\label{ftdataset}
\begin{tabular}{|c|c|}
\hline
   & \\
\hline
no segmentation  & 弊喇,好似有少少喉嚨痛添! \\
segmentation  & 弊 喇 , 好似 有 少 少 喉嚨痛 添 ! \\
segmentation + pinyin  & bì lǎ , hǎosì yǒu shǎo shǎo hóulóngtòng tiān ! \\
\hline
\end{tabular}
\end{table}

We also tested the both dimesion size from 8-300, and loss function including hs (hierarchical softmax), ns (negative sampling), softmax.


\section{Siamese-CBOW}

We use Siamese-CBOW with the default parameters that the authors suggested. 
The dimension of the vector is 100, and update algorithm is ada delta. 
We ran it with epoch 5 and 10 separately without any pre-trained word embeddings, so it generates the word embeddings from scratch.
Due to low performance of the trial without pre-trained word embeddings, we also tried to use Gensim to generate the pre-train word embeddings from our dataset.
With pre-trained word embeddings, we tried epoch 10 to run. After the vectors are generated, we use Linear SVC to classify the results. 

\begin{table}[]
\centering
\caption{Results: the best accuracy of different models}
\label{resultAll}
\begin{tabular}{|c|c|}
\hline
Tf-IDF   & 0.44 (± 0.04) \\
PVDM(dbow) & 0.40    \\
Fasttext &  0.51   \\
Fasttext(Pinyin) &  0.51  \\
Siamese-CBOW(10) with pre-train & 0.45 (± 0.02) \\

\hline
\end{tabular}
\end{table}

\begin{table}[]
\centering
\caption{Fasttext}
\label{fasttext}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
   & 8 & 12 & 16 & 32 & 64 \\
\hline
no segmentation  & 0.369 & 0.375 & 0.389 & 0.372 & 0.368 \\
segmentation  & 0.515 & 0.515 & 0.514 & 0.516 & 0.513 \\
pinyin  & 0.513 & 0.518 & 0.516 & 0.517 & 0.51 \\
\hline
\end{tabular}
\end{table}


\begin{table}[]
\centering
\caption{Results: the accuracy of Siamese-CBOW}
\label{resultSCBOW}
\begin{tabular}{|c|c|}
\hline
Siamese-CBOW(5) & 0.41 (± 0.04) \\
Siamese-CBOW(10) & 0.39 (± 0.03) \\
Siamese-CBOW(10) with pre-train & 0.45 (± 0.02) \\
\hline
\end{tabular}
\end{table}

\begin{table}[]
\centering
\caption{Result of PVDM}
\label{resultPVDM}
\begin{tabular}{|c|c|c|c|}
\hline
      & Test set & Training Set \\
\hline
DM/C  & 0.384 &  0.384 \\
DM/M &  0.38  & 0.436 \\
DBOW &  0.404  & 0.457 \\
\hline
\end{tabular}
\end{table}

\section{Experimental Results}

It took around 2 days with GPU NVIDIA TITAN X (PASCAL) to finish Siamese-CBOW word embeddings training. The performance with epoch 5, 10 are below the baseline. 
The other models were finished with CPU within hours.

Figure \ref{resultSCBOW} and Figure \ref{resultPVDM} show the accuracy for Siamese-CBOW and PVBM separately. 
In the experiment of Siamese-CBOW, we found that the result without pre-trained embedding of Siamese-CBOW is below the baseline. We trained the original dataset with Gensim word2vec as pre-trained embeddings.
The result is improved by the pre-trained embeddings, but it is not significant statistically from the baseline. 
In the result of PVDM, DBOW performs better than the other two DM models by little. 
The overall accuracy is a little below of baseline.


\begin{figure}[h]
    \centering
	\includegraphics[width=1\linewidth]{tf-idf}
    \caption{The confusion matrix for TF-IDF+ SVM}
    \label{confusion1}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}[b]{1\textwidth}
   \includegraphics[width=.9\linewidth]{sc-ratio}
   \caption{}
   \label{confusion2} 
\end{subfigure}

\begin{subfigure}[b]{1\textwidth}
   \includegraphics[width=.9\linewidth]{siamese-cbow-pretrained}
   \caption{}
   \label{confusion5}
\end{subfigure}
\caption[Confusion Matrix of Siamese-CBOW]{(a) The training set without pre-trained embedding,
(b) The training set with pre-trained embedding
}
\end{figure}



\begin{figure}
\centering
\begin{subfigure}[b]{1\textwidth}
   \includegraphics[width=.9\linewidth]{fasttext-All_seg_d300_w20_hs}
   \caption{}
   \label{confusion3} 
\end{subfigure}

\begin{subfigure}[b]{1\textwidth}
   \includegraphics[width=.9\linewidth]{All_seg_d200_w15_softmax_pinyin}
   \caption{}
   \label{confusion4}
\end{subfigure}
\caption[Confusion matrix of FastText]{(a) segmented dataset trained with dimension=300, window size=20, loss function hierarchical softmax,
(b) pinyin dataset with dimension=200, window size=15, and loss function = softmax
}
\end{figure}

We also look into the classification results with confusion matrices. 
In Figure \ref{confusion1} and Figure \ref{confusion2}, we can see that the 2 major classes give the best accuracy while the test results skewed into these 2 major classes as well.  
The ANGER, DISGUST, and FEAR are more likely to be classified as SAD. It is reasonable that those posts contain more negative sentiment.
The tendency in the Siamese-CBOW result is more obvious. It classified no entry into some rarely used classes, which may also result from its low performance.

We also evaluated all classification details of FastText experiments.
In the confusion matrix for FastText in Figure \ref{confusion4}, it shows different tendency for Pinyin dataset classification. 
The accuracy for SURPRISE is higher than SAD. It indicates that the SURPRISE may be modeled better than SAD, which contains more samples.
Figure \ref{confusion3} is quite similar with confusion matrices in other experiments, where most test data are classified as JOY and SAD, because the two classes contain the majority of the posts.
Most of the results of the segmented dataset are alike despite of the settings of loss function and dimension.
