\chapter{Discussion}

The result shows that FastText can archive better accuracy in general way. We also compare the result between different models.

\section{Discussion}


For the baseline, though TF-IDF it can archive the accuracy about 0.44(±0.04). 
The most distinguishable features they use are some rarely used terminology. 
Since we only removed the duplicated post roughly, it may still suffer from the duplicated post from different sources with certain rarely-used words. 
In general, the model is not general enough, it may not be applicable when the data set changed. 

Additionally, compared to other methods, those approaches convert the sentence to vectors with lower dimensions.
Theoretically, it may benefit the classifier with more dense presentation. Sparse matrix cause much more time and space to train.
With such high dimension vectors, it is unlikely to apply the non-linear classifier.

Generally, FastText can retrieve the better accuracy with segmentation inclusion. 
With conversion to Pinyin, it also achieves the similar accuracy. Though, we tried the different settings for FastText,
the accuracy is not different significantly despite of the various settings of loss function, window size and dimensions. 
In the comparison set, segmented dataset outperforms the one without segmentation. 
It suggested that the term itself may be more meaningful than a single character. And it also took much less time than that of other implementatons.

\section{Baseline}

We used TF-IDF as baseline, the result is 0.44(±0.04), which is statistically greater than the random result in 6 classes problem, 
even we consider the skewed classes.  We tried to evaluate the feature that TF-IDF use to classify as Table \ref{table:featureoftfidf}. 
Though some term here seemed related like, 2nd place in ANGER is 造謠 (create humor),3rd one is 恶浪(violent billow). 
The 2nd place in FEAR is 谋财害命(commit murder for money). The most term is not so related intuitively. 

Most of words in each classes actually are not used so commonly. It is contributed by the problem of IDF, which may overweight some rarely-used words.
The other problem is that we observed the netizen keep posting something quite similar like advertisement, joke or news..., 
Though their content are not the same exactly, the words or terminology they used may be quite common. 
Besides TF-IDF, most algorithms assume that the articles themselves are distinct from others. In real world, it is not a common case except some well-organized website articles.
We may require some extra preprocess procedure to handle to prevent intentionally repeated posts.

Some publications\cite{Chen2013TUP251293825} shows that the posters may use some morphs to avoid censorship, which we can't evaluate how these words contributes to the sentiment analysis. 
However, the similarity of morphs is quite easy to identified by Word2vec with proper context. 
It is relatively difficult to identify in TF-IDF model. 

\begin{table}[]
\centering
\caption{The feature extracted from TF-IDF}
\label{featureoftfidf}
\begin{tabular}{llllll}
ANGER	& DISGUST	& FEAR	& JOY	& SAD	& SURPRICE \\
想瘦:4.40	&抬杠:4.68	&含铅:4.66	&余则:7.53	&grunewald:4.68	&印钞机:5.16 \\
造謠:4.05	&赃款:3.79	&谋财害命:4.59	&符离:4.73	&节省时间:4.54	&绝密:4.23\\
恶浪:3.55	&由此看来:3.66	&表同情:3.98	&驻守:4.01	&张春桥:4.21	&54:03.7\\
顶缸:3.51	&淫妇:3.52	&经济体:3.86	&神明:3.72	&前导:3.83	&清福:3.60\\
银行利息:3.32	&觀天象:3.42	&可憐見:3.43	&asce:3.56	&已閱:3.35	&任免:3.24\\
卫冕冠军:3.24	&解除:3.38	&突如其来:3.35	&三元里:3.44	&q1050505041:3.34	&karei:3.07\\
落水狗:3.23	&上刑:3.35	&万念俱灰:3.11	&何苦来哉:3.44	&杀伐:3.29	&一桩:3.02\\
剖腹自杀:3.03	&矢泽爱:3.32	&供应站:3.03	&太妙了:3.34	&离世:3.28	&sikucd:2.97\\
剥下:3.01	&超现实主义:3.29	&勞資:2.98	&两面三刀:3.28	&噴火龍:3.27	&touchsmart610:2.91\\
多吉:2.98	&美國使館:3.06	&深红:2.90	&slient:3.25	&查無此人:3.18	&翻筋斗:2.82
\end{tabular}
\end{table}

\section{Siamese-CBOW}

The Siamese-CBOW, the performance is below the baseline. We tried evaluate the model it trained, it seemed it is not converged enough. 
The word embedding is not converted correctly. For example, we use "我" (I) to query in the word-embedding generated by Siamese-CBOW 5 epoch. 
The related words it show are 扙(hurt, rarely-used word),第四节 (the fourth quarter) and 賈寶玉(the name). With 10 epoch word embedding, the  
related words are 几家 (Some homes), 峯(peek), and 速成班 (rapid-archieve class). It seemed there is no sign of converge.

And in the confusion matrix, we found the most tested result fall into two major classes.
In the original paper, the dataset they conduected is Toronto Books, which contains novels,
 therefore the sematics of the sentences may be more highly coherent with previous sentence and next one. 
However, the property of the dataset should not affect the word embedding it trains. Since the proper word embedding is available with other implementations.

Using some pre-trained embeddings may help to deal with such situation. Another drawback is that Siamese-CBOW does not support the feature like subword information.
It means if the words if absent from its training dataset, it would be consider non-existent at all. Conducting the vocabulary expansion like that in Skip-Thoughts may assist the problem.


\section{PVDM vs. FastText}

In paragraph vector experiment, the result shows that DBOW produced best accuracy among 3 models. In the original paper, the author suggested that the DM is consistently better than DBOW
, and that the sum version of DM is often better than concatenation. 
So far, it's not clear under what condition that DBOW outperform the DM model. We tried leverge the model it built. 
We fetched most similar word of I (我) as Table \ref{table:doc2vec}. Surprisingly, the similar words of DBOW are all not related words. 
Both DM/C and DM/M generated better results, which top 10 related words are synonyms of I.

\begin{table}[]
\centering
\caption{The most similar 5 words of I (我) in 2 models}
\label{doc2vec}
\begin{tabular}{llll}
      & DM/C & DBOW & DM/M \\
1 & 俺 &  三条  & 偶\\
2 & 偶  & 田徑運動 & 他\\
3 & 老子  & 温暖人间 & 俺\\
4 & 哀家  & youtudou & 我们\\
5 & 皮下  & 化妝水 & 她
\end{tabular}
\end{table}

\section{Subword Information}

According to the paper, the feature subword information can compensate the insufficience of word embeddings. 
We tried to evaluate if the features work in pinyin as well, which means sentimatics can be inferred from the pronunciation. 
Although we converted the dataset to pinyin, the accuracy is not significantly different from the original accuracy. 
Intuitively, pinyin is less readible to native speaker and inreversible to the characters, since the multiple characters own the same pronunciation. 
Chinese contains less syllables and more homonyms than English do. In the original paper\cite{bojanowski2016enriching}, they evaluate the effectiveness in various languages like Arabic, Czech, German, English...etc.
All of them belong to phonography, and most languages belong to phonography. While Chinese belongs to logogram specially. So far, we are still hard to quantize the effectiveness conducted in Chinese.

In the \cite{DBLP:journals/corr/MikolovLS13}, it provides similar function to compensate the words absent from training set. 
It employs similarity of pre-trained word-embedding, rather than the similarity of n-gram characters features. 
There are also some differnet approaches like morphologically annotated data, whcih were introduced by Cotterell and Schütze (2015).
It may be a good topic to evaluate the difference of these approaches. 

\section{Conclusion}

We demonstrated the various modern methods on the Chinese corpus, and it indicated that some models like FastText are invariant to language property. 
In general, most models improve the sematic analysis compared with tranditional TFIDF, and it is also more effient to train with vectors.

Most methods are developed with English property, so segmentation plays a crucial role to make the Chinese posts look like English.
 But the segmentation may also contribute something wrong. Though FastText also can be conducted with non-segmented sentences, it performed worse due to the nature of word embedding.
Since the word2vec can distinguish the term by its nature, there may be some way to train the Chinese corpus without segmentation.

