\chapter{Discussion}

The result shows that FastText can archive better accuracy in general way.

\section{Discussion}


For the baseline, though TF-IDF it can archive the accuracy about 0.44(±0.04). 
The most distinguishable features they use are some rarely used terminology. 
Since we only removed the duplicated post roughly, it may still suffer from the duplicated post from different sources with certain rarely-used words. 
In general, the model is not general enough, it may not be applicable when the data set changed.

Generally, FastText can get the better accuracy , even converting the posts to pinyin,
it also achieves the same accuracy. Though, we tried the different settings for FastText,
the accuracy is not different significantly despite of the various settings of loss function,window size and dimension. In the comparison set, segmented data set outperforms the one without segmentation. 
It suggested that the term itself is more meaningful than a single character. And it also took much less time than that of other algorithms.

The Siamese-CBOW, the performance is below the baseline. 
We tried evaluate the model it trained, it seemed it is not converged enough. 
The word embedding is not converted correctly. And in the confusion matrix, we found the most tested result fall into two major classes.
In the original paper, the dataset they used is Toronto Books, which contains novels,
 therefore the sematics of the sentences may be more coherent with previous sentence and next one. Using some pre-trained embedding may help to deal with such situation. 

\section{Baseline}

We used TFIDF as baseline, the result is 0.44(±0.04), which is statistically greater than the random result in 6 classes problem, 
even we consider the skewed classes.  We tried to evaluate the feature that TF-IDF use to classify as Table . 
Most of words in each classes actually are not used so commonly. It is contributed by the problem of IDF, which may overweight some rarely-used words.
The other problem is that we observed the netizen keep posting something quite similar like advertisement, joke or news..., 
Though their content are not the same extractly, the words or terminology they used may be quite common. 
Besides TFIDF, most algorithms assume that the articles themselves are distinct from others. In real world, it's not a common case except some well-organized website articles.

\section{PVDM vs. FastText}

In paragraph vector experiment, the result shows that DBOW produced best accuracy among 3 models. In the original paper, the author suggested that the DM is consistently better than DBOW
, and that the sum version of DM is often better than concatenation. 
So far, it's not clear under what condition that DBOW outperform the DM model. We tried leverge the model it built. 
We fetched most similar word of I (我) as Table \ref{table:doc2vec}. Surprisingly, the similar words of DBOW are all not related words. 
Both DM/C and DM/M generated better results.

\begin{table}[]
\centering
\caption{The most similar 5 words of I (我) in 2 models}
\label{doc2vec}
\begin{tabular}{llll}
      & DM/C & DBOW & DM/M \\
1 & 俺 &  三条  & 偶\\
2 & 偶  & 田徑運動 & 他\\
3 & 老子  & 温暖人间 & 俺\\
4 & 哀家  & youtudou & 我们\\
5 & 皮下  & 化妝水 & 她
\end{tabular}
\end{table}

\section{Conclusion}

We demonstrated the various modern methods on the Chinese corpus, and it indicated that some models like FastText are invariant to language property. 
In general, most models improve the sematic analysis compared with tranditional TFIDF.

Most methods are developed with English property, so segmentation plays a crucial role to make the Chinese posts look like English.
 But the segmentation may also contribute something wrong. Though FastText also can performed with non-segmented sentences, it performed worse due to  the nature of word embedding.

Some publications\cite{Chen:2013:TUP:2512938.2512940} shows that the posters may use some morphs to avoid censorship, which we can't evaluate how these words contributes to the sentiment analysis. 
 However, the similarity of morphs is quite easy to identified by Word2vec. 